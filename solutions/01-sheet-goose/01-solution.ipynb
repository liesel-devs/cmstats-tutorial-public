{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format: html\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\solution{01}\n",
        "\n",
        "## Sheet 01 - Solution\n",
        "\n",
        "First, we import the Python libraries we need to solve the exercise and define the same data and log unnormalized posterior as in Exercise 2 on Sheet 00."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import NamedTuple, TypeAlias\n",
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "\n",
        "import liesel.goose as gs\n",
        "\n",
        "Array: TypeAlias = jax.Array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "mu = -1.4\n",
        "sigma = 0.8\n",
        "n = 10\n",
        "\n",
        "dist = tfp.distributions.Normal(mu, 0.8**2)\n",
        "obs = dist.sample(n, seed=key)\n",
        "\n",
        "class Params2(NamedTuple):\n",
        "    mu: float\n",
        "    log_sigma: float\n",
        "    \n",
        "def log_prior2(params: Params2) -> Array:\n",
        "    sigma = jnp.exp(params.log_sigma)\n",
        "    lp_mu = tfp.distributions.Normal(0, 10).log_prob(params.mu,)\n",
        "    lp_sigma =  tfp.distributions.HalfCauchy(0, 1).log_prob(sigma) + params.log_sigma\n",
        "    return lp_mu + lp_sigma\n",
        "\n",
        "def log_likelihood2(params: Params2, obs: Array) -> Array:\n",
        "    sigma = jnp.exp(params.log_sigma)\n",
        "    dist = tfp.distributions.Normal(params.mu, sigma)\n",
        "    lps = dist.log_prob(obs)\n",
        "    return jnp.sum(lps)\n",
        "\n",
        "def log_uposterior2(params: Params2, obs: Array) -> Array:\n",
        "    return log_prior2(params) + log_likelihood2(params, obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set up the sampler with Goose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# the partial application returns a function that only takes the parameters\n",
        "model_log_prob = partial(log_uposterior2, obs=obs)\n",
        "\n",
        "builder = gs.EngineBuilder(seed=0, num_chains=3)\n",
        "interface = gs.NamedTupleInterface(model_log_prob)\n",
        "builder.set_model(interface)\n",
        "builder.set_initial_values(Params2(0.0, 0.0))\n",
        "builder.set_duration(warmup_duration=500, posterior_duration=1000)\n",
        "builder.add_kernel(\n",
        "    gs.HMCKernel([\"mu\", \"log_sigma\"])\n",
        ")\n",
        "engine = builder.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "engine.sample_all_epochs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we see some of Goose output. It tells us, when it enters a new epoch and how many errors have been encountered during sampling in this Epoch. The term error, can also refer to a warning that the transition kernel issues. \n",
        "\n",
        "Goose also provides tools to summarize the results, including summaries of the errors and plots for diagnostics. Compared with Sheet 00, we have here a posterior mean estimate and uncertainty quantification. In the error summary table, we find that all errors referred to divergent transition during warmup. This is expected to happen during the auto-tuning phases of the HMC sampler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = engine.get_results()\n",
        "gs.Summary(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs.plot_trace(results)\n",
        "gs.plot_density(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we have specified the model in terms of $\\log(\\sigma)$ instead of $\\sigma$, we get the estimates also for this quantity. We are usually interested in the those values for $\\sigma$. Just applying $\\exp$ to the posterior summaries is not valid. Instead, we apply the transformation to all posterior samples and include them in the summary. When using Liesel's model library, this transformation will be simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "additional_chain = {'sigma': jnp.exp(results.get_posterior_samples()['log_sigma'])}\n",
        "gs.Summary(results, additional_chain=additional_chain)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}