{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Solutions Sheet 02\n",
        "format: \n",
        "  html:\n",
        "    standalone: true\n",
        "    toc: true\n",
        "execute: \n",
        "  cache: true\n",
        "  echo: fenced\n",
        "engine: knitr\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< include ../math.qmd >}}\n",
        "\n",
        "\\solution{02}\n",
        "\n",
        "\n",
        "On Google Colab, you can install the required libraries with the following\n",
        "commands:\n",
        "\n",
        "```\n",
        "!apt install libgraphviz-dev\n",
        "!pip install pygraphviz\n",
        "!pip install liesel\n",
        "!pip install plotnine\n",
        "```\n",
        "\n",
        "## Exercise 1: Statistical models as directed acyclic graphs\n",
        "\n",
        "- To get an overview of a statistical model, it can be helpful to\n",
        "represent them as directed acyclic graphs. In our usage, they consist of two main building blocks:\n",
        "    - Nodes: The variables that come up in the model\n",
        "    - Edges: The connections between the variables. These edges are directed, that means they represent the flow of information from one node to another.\n",
        "    - The graph is **acyclic**, which means no node can become it's own input (or the input to its inputs).\n",
        "\n",
        "We differentiate nodes based on two concepts:\n",
        "\n",
        "- **Strong** and **weak** nodes: Strong nodes introduce new information into the graph, while weak nodes are deterministically calculated from other nodes in the graph.\n",
        "- **Random** and **nonrandom** nodes. Nodes with an associated probability distribution are random. Other nodes are nonrandom.\n",
        "\n",
        "We also differentiate between two types of edges:\n",
        "\n",
        "- Used in **value**: For edges that represent the flow of information as inputs for deterministic computations.\n",
        "- Used in **distribution**: For edges that represent the flow of information as inputs to probability distributions.\n",
        "\n",
        "### Subtask a): The first graph\n",
        "\n",
        "In this model, we have the following nodes:\n",
        "\n",
        "- $\\text{rent}$: The observed values and observation model for our response variable.\n",
        "  - This node is *strong*, because the observed values enter the graph with this node - they are not calculated.\n",
        "  - This node is *random*, because it has an associated probability distribution.\n",
        "- $\\mu$: The mean of the response distribution.\n",
        "  - This node is *strong*, because the value of $\\mu$ enters the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "- $\\sigma^2$: The variance of the response distribution.\n",
        "  - This node is *strong*, because the value of $\\sigma^2$ enters the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "- $\\sigma$: The scale of the response distribution. We include it here mainly for illustration.\n",
        "  - This node is *weak*, because the value of $\\sigma$ is calculated as the square root of $\\sigma^2$.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "\n",
        "\n",
        "![](img/ex01-a-graph.png)\n",
        "\n",
        "\n",
        "### Subtask b): Adding a regression model\n",
        "\n",
        "In this model, we observe the following changes:\n",
        "\n",
        "- $\\beta_0$: The model intercept is added as a new node.\n",
        "  - This node is *strong*, because the value of $\\beta_0$ enters the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "- $\\beta_1$: The regression coefficient for the covariate $\\text{area}$ is added as a new node.\n",
        "  - This node is *strong*, because the value of $\\beta_0$ enters the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "- $\\text{area}$: The observed values of the covariate $\\text{area}$\n",
        "  - This node is *strong*, because the values of $\\text{area}$ enter the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "- $\\mu$: The mean of the response distribution is now a deterministic function of\n",
        "  other nodes in the graph through the model $\\mu_i = \\beta_0 + \\beta_1 \\text{area}_i$. \n",
        "  Thus, it changes from a strong node into a *weak* node. It remains a nonrandom node.\n",
        "\n",
        "\n",
        "![](img/ex01-b-graph.png)\n",
        "\n",
        "### Subtask c): Adding a prior\n",
        "\n",
        "In this model, we observe the following changes:\n",
        "\n",
        "- $\\sigma^2$: The variance of the response distribution now has its own \n",
        "  probability distribution, the inverse gamma prior, so that it changes from\n",
        "  a nonrandom to a *random* node. It remains a strong node.\n",
        "- $a$: The prior shape (=concentration) of the inverse gamma prior for $\\sigma^2$.\n",
        "  - This node is *strong*, because the value of $a$ enters the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "- $b$: The prior scale of the inverse gamma prior for $\\sigma^2$.\n",
        "  - This node is *strong*, because the value of $b$ enters the graph with this node.\n",
        "  - This node is *nonrandom*, because it does not have a probability distribution.\n",
        "\n",
        "\n",
        "![](img/ex01-c-graph.png)\n",
        "\n",
        "\n",
        "## Exercise 2: Your first Liesel model\n",
        "\n",
        "Liesel is built with the graph representation in mind, providing you the\n",
        "basic building blocks. The four fundamental blocks are:\n",
        "\n",
        "- `lsl.Var`: A statistical variable, always shows up in the model graph. \n",
        "   - A `lsl.Var` object is *strong* and *nonrandom* by default. \n",
        "   - It can be associated with a probability distribution via `lsl.Dist`, making it *random*.\n",
        "   - It can wrap a function via `lsl.Calc`, making it *weak*.\n",
        "- `lsl.Dist`: Wraps a probability distribution.\n",
        "- `lsl.Calc`: Wraps a function. \n",
        "   - Shows up in the model graph only if wrapped by a `lsl.Var`.\n",
        "- `lsl.Data`: Holds constant auxiliary data, for example for storing values that\n",
        "   are precomputed for convenience or efficiency, but not of major modeling\n",
        "   interest.\n",
        "\n",
        "Liesel also provides two convenience functions:\n",
        "\n",
        "- `lsl.obs`: Initializes a `lsl.Var` and sets the `lsl.Var.observed` flag to `True`. \n",
        "   This makes sure that, if the variable is random, its log probability/density\n",
        "   is added to the model's log likelihood.\n",
        "- `lsl.param`: Initializes a `lsl.Var` and sets the `lsl.Var.parameter` flag to\n",
        "   `True`. This makes sure that, if the variable is random, its log \n",
        "   probability/density is added to the model's log prior.\n",
        "\n",
        "\n",
        "\n",
        "### Exercise 2a): A minimal model\n",
        "\n",
        "First, we import the data. Here, we transform the values directly to the \n",
        "data type `float32`, since JAX works with 32-bit floats. Liesel would also\n",
        "convert these values automatically, but we like to be explicit here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "rent99 = pd.read_csv(\"https://s.gwdg.de/mzAkHV\")\n",
        "area = rent99.area.to_numpy(\"float32\")\n",
        "rent = rent99.rent.to_numpy(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set up the leaf nodes. Even though they are not random, we initialize\n",
        "the mean and variance as parameters. Both receive a value and a name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import liesel.model as lsl\n",
        "\n",
        "mu = lsl.param(0.0, name=\"mu\")\n",
        "sigma_sq = lsl.param(10.0, name=\"sigma_sq\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we use our first calculator. The `lsl.Calc` class receives the function\n",
        "to execute as its first argument, followed by the inputs. We use pure,\n",
        "jittable functions in `lsl.Calc` objects, and can very often simply resort\n",
        "to using functions available from `jax.numpy`.\n",
        "\n",
        "We wrap the calculator in a `lsl.Var`, because we want the `\"sigma\"` node to \n",
        "show up in the model graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "sigma = lsl.Var(\n",
        "  lsl.Calc(jnp.sqrt, sigma_sq), name=\"sigma\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set up the response node with its probability distribution. We actually\n",
        "start with the distribution. The `lsl.Dist` class wraps probability distributions\n",
        "that follow the `tensorflow_probability` interface.\n",
        "The `lsl.Dist` instance\n",
        "is then fed as the second argument to `lsl.obs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow_probability.substrates.jax.distributions as tfd\n",
        "\n",
        "y_dist = lsl.Dist(tfd.Normal, loc=mu, scale=sigma)\n",
        "y = lsl.obs(rent, y_dist, name=\"rent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our first model is almost complete. We bring everything together by initializing\n",
        "a `lsl.GraphBuilder` and adding our response node. Since all other nodes in the\n",
        "graph can be found as inputs to this response node, we only need the \n",
        "GraphBuilder to know about this one. All other nodes will be found \n",
        "automatically. We finish this task by building our model and plotting our graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gb = lsl.GraphBuilder().add(y)\n",
        "\n",
        "model = gb.build_model()\n",
        "lsl.plot_vars(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the log probability, log likelihood and log prior of the\n",
        "fully built model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.log_prob)\n",
        "print(model.log_lik)\n",
        "print(model.log_prior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the log probability/density of all of our variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(mu.log_prob)\n",
        "print(sigma_sq.log_prob)\n",
        "print(sigma.log_prob)\n",
        "print(y.log_prob) # \n",
        "print(y.log_prob.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2b): Adding a regression model\n",
        "\n",
        "The main difference here lies in the way we define the response's mean. \n",
        "We have three new nodes: the observed values of $\\text{area}$, the\n",
        "intercept $\\beta_0$ and the coefficient $\\beta_1$.\n",
        "\n",
        "We then define $\\mu$ as the output of a short function and wrap it with \n",
        "a `lsl.Calc` and a `lsl.Var`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = lsl.obs(area, name=\"area\")\n",
        "\n",
        "b0 = lsl.param(0.0, name=\"b0\")\n",
        "b1 = lsl.param(0.0, name=\"b1\")\n",
        "\n",
        "def linear_model(x, b0, b1):\n",
        "    return b0 + x*b1\n",
        "\n",
        "mu = lsl.Var(\n",
        "  lsl.Calc(linear_model, x=x, b0=b0, b1=b1), name=\"mu\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the model stays the same, so we can copy from above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sigma_sq = lsl.param(10.0, name=\"sigma_sq\")\n",
        "sigma = lsl.Var(lsl.Calc(jnp.sqrt, sigma_sq), name=\"sigma\")\n",
        "\n",
        "y_dist = lsl.Dist(tfd.Normal, loc=mu, scale=sigma)\n",
        "y = lsl.obs(rent, y_dist, name=\"rent\")\n",
        "\n",
        "gb = lsl.GraphBuilder().add(y)\n",
        "model = gb.build_model()\n",
        "lsl.plot_vars(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2c): Adding a prior\n",
        "\n",
        "The main difference here lies in the way we define the response's variance.\n",
        "That is, we now set up a prior for the `sigma_sq` node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = lsl.Var(0.01, name=\"a\")\n",
        "b = lsl.Var(0.01, name=\"b\")\n",
        "\n",
        "sigma_sq_dist = lsl.Dist(tfd.InverseGamma, concentration=a, scale=b)\n",
        "sigma_sq = lsl.param(1.0, sigma_sq_dist, name=\"sigma_sq\")\n",
        "\n",
        "sigma = lsl.Var(lsl.Calc(jnp.sqrt, sigma_sq), name=\"sigma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the model stays the same, so we can copy from above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = lsl.obs(area, name=\"area\")\n",
        "\n",
        "b0 = lsl.param(0.0, name=\"b0\")\n",
        "b1 = lsl.param(0.0, name=\"b1\")\n",
        "\n",
        "def linear_model(x, b0, b1):\n",
        "    return b0 + x*b1\n",
        "\n",
        "mu = lsl.Var(lsl.Calc(linear_model, x=x, b0=b0, b1=b1), name=\"mu\")\n",
        "\n",
        "y_dist = lsl.Dist(tfd.Normal, loc=mu, scale=sigma)\n",
        "y = lsl.obs(rent, y_dist, name=\"rent\")\n",
        "\n",
        "gb = lsl.GraphBuilder().add(y)\n",
        "model = gb.build_model()\n",
        "lsl.plot_vars(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the log probability, log likelihood and log prior of the\n",
        "fully built model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.log_prob)\n",
        "print(model.log_lik)\n",
        "print(model.log_prior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the log probability/density of all of our variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(mu.log_prob)\n",
        "print(sigma_sq.log_prob)\n",
        "print(sigma.log_prob)\n",
        "print(y.log_prob) # \n",
        "print(y.log_prob.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Manipulate a Graph\n",
        "\n",
        "### Exercise 3a): Update existing prior\n",
        "\n",
        "We can simply override the `lsl.Var.value` attribute of the relevant nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.log_prob)\n",
        "a.value = 2.0\n",
        "b.value = 0.5\n",
        "print(model.log_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3b): Replace existing prior\n",
        "\n",
        "- A `lsl.Model` object assumes a static graph. That means, the values of nodes\n",
        "  can change, but the nodes themselves stay fixed.\n",
        "- To update the graph, we therefore extract the nodes and variables from our\n",
        "  model and set up a new graph using our response node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "_, vars_ = model.pop_nodes_and_vars()\n",
        "\n",
        "gb = lsl.GraphBuilder().add(vars_[\"rent\"])\n",
        "gb.plot_vars()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we are replacing a weak, nonrandom `lsl.Var` with a strong, random `lsl.Var`.\n",
        "- the existing $\\sigma$ is weak and nonrandom, because it is the square root\n",
        "  of the strong, random node $\\sigma^2$.\n",
        "- the new $\\sigma$ node will be strong and random, because we include it \n",
        "  directly without referring to $\\sigma^2$, and because we place the prior\n",
        "  directly on $\\sigma$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hc_prior_scale = lsl.Var(25.0, name=\"hc_prior_scale\")\n",
        "\n",
        "sigma_dist = lsl.Dist(tfd.HalfCauchy, loc=0.0, scale=hc_prior_scale)\n",
        "\n",
        "sigma_hc = lsl.param(1.0, sigma_dist, name=\"sigma_hc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GraphBuilder then allows us to replace the node in question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gb.replace_var(sigma, sigma_hc)\n",
        "gb.plot_vars()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3c): Transform a parameter\n",
        "\n",
        "- To transform a parameter, we again use functionality offered by TensorFlow, namely\n",
        "its bijector interface. \n",
        "- It offers a wide range of useful bijective functions that we can use a inverse link functions. \n",
        "- The bijectors are named after their \"forward\" transformation, i.e. the *inverse* link function.\n",
        "- In this case, since we want to use a logarithmic transformation of the parameter $\\sigma$,\n",
        "  we need the `Exp` bijector.\n",
        "- The `lsl.GraphBuilder` offers the function `lsl.GraphBuilder.transform`, which\n",
        "  will... \n",
        "  \n",
        "  i) ... create the new transformed node and turns the original node into an \n",
        "     appropriate calculator node to hold the output of the calculation.\n",
        "  ii) ... automatically apply the change-of-variables theorem for us such that \n",
        "     the prior will be defined for the transformed variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
        "\n",
        "gb.transform(sigma_hc, tfb.Exp)\n",
        "gb.plot_vars()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Sample from the posterior using Goose\n",
        "\n",
        "Here, we apply the knowledge from Exercise Sheet 01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = gb.build_model()\n",
        "interface = gs.LieselInterface(model)\n",
        "\n",
        "eb = gs.EngineBuilder(seed=1, num_chains=4)\n",
        "eb.add_kernel(gs.NUTSKernel([\"b0\", \"b1\"]))\n",
        "eb.add_kernel(gs.IWLSKernel([\"sigma_hc_transformed\"]))\n",
        "\n",
        "eb.set_duration(warmup_duration=1000, posterior_duration=1000)\n",
        "eb.set_model(interface)\n",
        "eb.set_initial_values(model.state)\n",
        "eb.set_engine_seed(seed=2)\n",
        "\n",
        "engine = eb.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "engine.sample_all_epochs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = engine.get_results()\n",
        "gs.Summary(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs.plot_trace(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: A semiparametric model in Liesel\n",
        "\n",
        "### Subtask a): Bayesian P-spline\n",
        "\n",
        "We start by representing $s(\\text{area}_i)$ with B-spline bases of order $3$\n",
        "and spline coefficients $\\beta_1, \\dots, \\beta_J$:\n",
        "$$\n",
        "s(\\text{area}_i) = \\sum_{j=1}^J B_j(\\text{area}_i)\\beta_j.\n",
        "$$\n",
        "For the spline coefficients $\\bsbeta = [\\beta_1, \\dots, \\beta_J]^T$, we define a second-order random walk prior,\n",
        "which is a partially improper multivariate normal prior of the form\n",
        "$$\n",
        "f(\\bsbeta | \\tau^2) \\propto \\left(\\frac{1}{\\tau^2}\\right)^{(L-2)/2}\\exp \\left( - \\frac{1}{2 \\tau^2} \\bsbeta^T \\bfK \\bsbeta \\right).\n",
        "$$\n",
        "Here, the penalty matrix $\\bfK$ is defined as the crossproduct of a \n",
        "second-differences matrix $\\bfD$ of dimension  $(J-2) \\times J$, so that \n",
        "$\\bfK = \\bfD^T\\bfD$ with\n",
        "$$\n",
        "\\bfD = \n",
        "\\begin{bmatrix}\n",
        "1 & -2 & 1 & 0 &\\cdots & 0 \\\\\n",
        "0 & 1 & -2 &1 &\\ddots  &  \\vdots\\\\\n",
        "\\vdots&\\ddots&\\ddots& \\ddots &\\ddots & 0\\\\\n",
        "0 & \\cdots & 0 & 1 & -2 &1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "The setup is completed by defining a hyperprior for the variance of the random\n",
        "walk, the inverse smoothing parameter $\\tau^2$. Here, we choose an inverse \n",
        "gamma prior:\n",
        "$$\n",
        "\\tau^2 \\sim \\mathcal{IG}(0.01, 0.01).\n",
        "$$\n",
        "\n",
        "### Subtask b): Model graph\n",
        "\n",
        "This is the model graph:\n",
        "\n",
        "![](img/ex05-graph.png)\n",
        "\n",
        "\n",
        "### Subtask c): Implementation in Liesel\n",
        "\n",
        "#### Spline coefficient subgraph\n",
        "\n",
        "Let us start with the subgraph that represents $\\bsbeta$. First, we define\n",
        "the hyperprior for $\\tau^2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = lsl.Var(0.01, name=\"a_tau\")\n",
        "b = lsl.Var(0.01, name=\"b_tau\")\n",
        "tau_sq_prior = lsl.Dist(tfd.InverseGamma, concentration=a, scale=b)\n",
        "tau_sq = lsl.param(10.0, tau_sq_prior, name=\"tau_sq\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we construct the penalty matrix $\\bfK$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nparam = 20\n",
        "D = jnp.diff(jnp.eye(nparam), n=2, axis=0)\n",
        "K = D.T @ D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can set up the partially improper normal prior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from liesel.distributions import MultivariateNormalDegenerate\n",
        "\n",
        "beta_prior = lsl.Dist(\n",
        "  MultivariateNormalDegenerate.from_penalty, loc=0.0, var=tau_sq, pen=K\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can set up the $\\bsbeta$ node, bringing everything together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta = lsl.param(jnp.zeros(20), beta_prior, name=\"beta\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Location subgraph\n",
        "\n",
        "Next, we import the prepared matrix of basis function evaluations using numpy.\n",
        "We also create a `lsl.Var` that holds the basis matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "basis_matrix = np.loadtxt(\"https://s.gwdg.de/LZnQMC\")\n",
        "\n",
        "area_basis = lsl.obs(basis_matrix, name=\"area_basis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we include the location calculator. This calculator implements\n",
        "$$\n",
        "\\bss(\\mathbf{area}) = \\bfB \\bsbeta,\n",
        "$$\n",
        "where $\\bfB$ is the matrix of basis function evaluations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mu = lsl.Var(\n",
        "  lsl.Calc(jnp.dot, area_basis, beta), name=\"mu\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scale subgraph\n",
        "\n",
        "This is just an application of what we did earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a_sigma = lsl.Var(0.01, name=\"a_sigma\")\n",
        "b_sigma = lsl.Var(0.01, name=\"b_sigma\")\n",
        "sigma_sq_dist = lsl.Dist(tfd.InverseGamma, concentration=a_sigma, scale=b_sigma)\n",
        "sigma_sq = lsl.param(10.0, sigma_sq_dist, name=\"sigma_sq\")\n",
        "sigma = lsl.Var(lsl.Calc(jnp.sqrt, sigma_sq), name=\"sigma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Response\n",
        "\n",
        "Everything now comes together in the response node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_dist = lsl.Dist(tfd.Normal, loc=mu, scale=sigma)\n",
        "y = lsl.obs(rent, y_dist, name=\"rent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gb = lsl.GraphBuilder().add(y)\n",
        "model = gb.build_model()\n",
        "lsl.plot_vars(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model\n",
        "\n",
        "We finally build the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.pop_nodes_and_vars()\n",
        "\n",
        "gb = lsl.GraphBuilder().add(y)\n",
        "gb.transform(tau_sq, tfb.Exp)\n",
        "gb.transform(sigma_sq, tfb.Exp)\n",
        "model = gb.build_model()\n",
        "lsl.plot_vars(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subtask d): Setting up the sampling engine\n",
        "\n",
        "Setting up the engine works pretty much like in the earlier examples.\n",
        "Just don't forget that $\\tau^2$ and $\\sigma^2$ are sampled \n",
        "after transformation here, so we have to append `\"_transformed\"` to the\n",
        "respective node names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eb = gs.EngineBuilder(seed=10, num_chains=4)\n",
        "\n",
        "eb.add_kernel(gs.NUTSKernel([\"beta\", \"tau_sq_transformed\"]))\n",
        "eb.add_kernel(gs.NUTSKernel([\"sigma_sq_transformed\"]))\n",
        "\n",
        "eb.set_duration(warmup_duration=1000, posterior_duration=1000)\n",
        "eb.set_model(gs.LieselInterface(model))\n",
        "eb.set_initial_values(model.state)\n",
        "\n",
        "engine = eb.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the engine has been set up, the sample from all epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "engine.sample_all_epochs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a first look at the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = engine.get_results()\n",
        "summary = gs.Summary(results)\n",
        "summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subtask e): Trace plots\n",
        "\n",
        "The trace plots look quite encouraging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gs.plot_trace(results, \"beta\", range(0, 9))\n",
        "gs.plot_trace(results, \"beta\", range(9, 20))\n",
        "gs.plot_param(results, \"tau_sq_transformed\")\n",
        "gs.plot_param(results, \"sigma_sq_transformed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subtask f): Plotting the posterior mean function\n",
        "\n",
        "The summary object gives us access to a number of useful summary statistics.\n",
        "We can see the names by printing out the keys of the `gs.Summary.quantities`\n",
        "dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "list(summary.quantities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use these summary statistics to extract some values for plotting. In \n",
        "addition to the mean, we use the highest posterior density interval here\n",
        "to quantify uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta_mean = summary.quantities[\"mean\"][\"beta\"]\n",
        "beta_hdi = summary.quantities[\"hdi\"][\"beta\"]\n",
        "\n",
        "s_mean = basis_matrix @ beta_mean\n",
        "s_hdi_lo = basis_matrix @ beta_hdi[0,:]\n",
        "s_hdi_hi = basis_matrix @ beta_hdi[1,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For plotting, we can use the Python library `plotnine`, which practically\n",
        "ports `ggplot2` from R to Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plotnine import ggplot, aes, geom_line, geom_ribbon\n",
        "\n",
        "(\n",
        "  ggplot()\n",
        "  + aes(area, s_mean)\n",
        "  + geom_line()\n",
        "  + geom_ribbon(aes(ymin = s_hdi_lo, ymax = s_hdi_hi), alpha = 0.2)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}