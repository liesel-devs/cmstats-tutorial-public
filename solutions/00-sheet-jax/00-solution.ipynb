{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format: html\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\solution{00}\n",
        "# Sheet 00 - Solutions\n",
        "\n",
        "First, we import the python libraries which we need to solve the exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from typing import NamedTuple, TypeAlias\n",
        "\n",
        "Array: TypeAlias = jax.Array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we generate some data. More precisely, we generate 10 observations from a Gaussian distribution with expectation $1.4$ and variance $0.8^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mu = -1.4\n",
        "sigma = 0.8\n",
        "n = 10\n",
        "obs = tfp.distributions.Normal(mu, 0.8**2).sample(n, seed=jax.random.PRNGKey(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 01\n",
        "\n",
        "Define functions for the log prior, log likelihood and log unnormalized posterior assuming known variance. Since $\\mu \\in R$, we do not need to perform a parameter transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def log_prior(params) -> Array:\n",
        "    return tfp.distributions.Normal(0, 10).log_prob(params)\n",
        "\n",
        "def log_likelihood(params, obs: Array) -> Array:\n",
        "    dist = tfp.distributions.Normal(params, sigma**2)\n",
        "    lps = dist.log_prob(obs)\n",
        "    return jnp.sum(lps)\n",
        "\n",
        "def log_uposterior(params, obs: Array) -> Array:\n",
        "    return log_prior(params) + log_likelihood(params, obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we use Newton's method to find the posterior mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# we start with a random guess\n",
        "params = jnp.array(0.0)\n",
        "\n",
        "# we only need the gradient and hessian wrt to the parameters,\n",
        "# the first argument of `log_uposterior_flat`\n",
        "dlp = jax.grad(log_uposterior, argnums=0)\n",
        "ddlp= jax.hessian(log_uposterior, argnums=0)\n",
        "\n",
        "# if we want, we can jit the functions\n",
        "jdlp = jax.jit(dlp)\n",
        "jddlp = jax.jit(ddlp)\n",
        "\n",
        "# we can now run the optimization\n",
        "# on the flattend parameter\n",
        "for i in (pb := tqdm(range(10))):\n",
        "    params = params - jdlp(params, obs) / jddlp(params, obs)\n",
        "    pb.set_description(f\"mu={params:.2f}, log_upost={log_uposterior(mu, obs):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xs = jnp.linspace(-3, 0, 100)\n",
        "lps = [log_uposterior(x, obs) for x in xs]\n",
        "plt.vlines(params, min(lps), max(lps), color=\"red\", linestyles=\"dashed\")\n",
        "plt.plot(xs, lps)\n",
        "plt.xlabel(\"mu\")\n",
        "plt.ylabel(\"log unnormalized posterior\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 02\n",
        "\n",
        "Here, we repeat the above. This time, the parameter class has the additional field `log_sigma`. We use `log_sigma` instead of `sigma`, since we need for Newton's algorithm that all parameters are defined on 'R'. Since out mathematical model is defined in terms of sigma, we need to apply the change of variable theorem to sigmas prior.\n",
        "\n",
        "When solving Exercise 02, we also opt to use a user-defined class to represent the vector. As you can see, it makes referring to the individual parameters less error-prone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Params2(NamedTuple):\n",
        "    mu: float\n",
        "    log_sigma: float\n",
        "    \n",
        "def log_prior2(params: Params2) -> Array:\n",
        "    sigma = jnp.exp(params.log_sigma)\n",
        "    lp_mu = tfp.distributions.Normal(0, 10).log_prob(params.mu,)\n",
        "    lp_sigma =  tfp.distributions.HalfCauchy(0, 1).log_prob(sigma) + params.log_sigma\n",
        "    return lp_mu + lp_sigma\n",
        "\n",
        "def log_likelihood2(params: Params2, obs: Array) -> Array:\n",
        "    sigma = jnp.exp(params.log_sigma)\n",
        "    dist = tfp.distributions.Normal(params.mu, sigma)\n",
        "    lps = dist.log_prob(obs)\n",
        "    return jnp.sum(lps)\n",
        "\n",
        "def log_uposterior2(params: Params2, obs: Array) -> Array:\n",
        "    return log_prior2(params) + log_likelihood2(params, obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is, however, not clear how a the gradient or Hessian are defined when using a function with a non-vector-valued argument. Luckily, `jax` provides a function that can use any pytree and convert it to a vector and back. This function is called `ravel_tree` and is found in the module `jax.flatten_tree`. We are going to use this functionality to define an additional unnormalized posterior function that works with an input vector and internally converts it back to our parameter type and calls `log_uposterior2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We need an example of our parameter to be able to use ravel_pytree\n",
        "params2 = Params2(mu=0.0, log_sigma=0.0)\n",
        "\n",
        "# ravel_pytree flattens the parameters (which could have a complex and\n",
        "# nested structure) into a single vector.\n",
        "# The function returns the flattend parameter vector and the inverse function\n",
        "# that takes a vector and returns an object of the parameter type.\n",
        "flat_params2, unravel_fn2 = ravel_pytree(params2)\n",
        "\n",
        "# now we define a function that takes a flat parameter vector and returns the\n",
        "# log unnormalized posterior internally, this function unravels the flat\n",
        "# parameter vector into the parameter object\n",
        "def log_uposterior2_flat(flat_params: Array, obs: Array) -> Array:\n",
        "    params: Params2 = unravel_fn2(flat_params)\n",
        "    return log_uposterior2(params, obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this optimzation, we use gradient ascent but determine the step-size automatically using the Hessian (see [website](https://calculus.subwiki.org/wiki/Gradient_descent_using_Newton%27s_method)). Note that the optimization operates on the flattened parameter vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# we start with a random guess\n",
        "params2 = Params2(mu=0.0, log_sigma=0.0)\n",
        "# and flatten it\n",
        "flat_params2, _ = ravel_pytree(params2)\n",
        "\n",
        "# we only need the gradient wrt to the parameters, the first argument\n",
        "dlp2 = jax.grad(log_uposterior2_flat, argnums=0)\n",
        "ddlp2 = jax.hessian(log_uposterior2_flat, argnums=0)\n",
        "\n",
        "# if we want, we can jit the functions\n",
        "jdlp2 = jax.jit(dlp2)\n",
        "jddlp2 = jax.jit(ddlp2)\n",
        "\n",
        "for i in (pb := tqdm(range(100))):\n",
        "    grad = jdlp2(flat_params2, obs)\n",
        "    hess = jddlp2(flat_params2, obs)\n",
        "    alpha = (grad ** 2).sum() / (grad.T @ hess @ grad)\n",
        "    flat_params2 = flat_params2 - alpha * grad\n",
        "\n",
        "    params2: Params2 = unravel_fn2(flat_params2)\n",
        "    pb.set_description(\n",
        "        f\"mu={params2.mu:.2f}, \"\n",
        "        f\"sigma={jnp.exp(params2.log_sigma):.2f}, \"\n",
        "        f\"log_upost={log_uposterior2(params2, obs):.2f}\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axs = plt.subplots(1, 2)\n",
        "xs = jnp.linspace(-3, 0, 100)\n",
        "lps = [log_uposterior2(Params2(mu=x, log_sigma=params2.log_sigma), obs) for x in xs]\n",
        "axs[0].vlines(params2.mu, min(lps), max(lps), color=\"red\", linestyles=\"dashed\")\n",
        "axs[0].plot(xs, lps)\n",
        "axs[0].set_xlabel(\"mu\")\n",
        "axs[0].set_ylabel(\"log unnormalized posterior\\n (conditioned on \\\\hat\\\\sigma)\")\n",
        "axs[0].text(-3, max(lps), \"mean(obs): {:.2f}\".format(jnp.mean(obs)), ha=\"left\")\n",
        "\n",
        "\n",
        "xs = jnp.linspace(-2, 1, 100)\n",
        "lps = [log_uposterior2(Params2(mu=params2.mu, log_sigma=x), obs) for x in xs]\n",
        "axs[1].vlines(params2.log_sigma, min(lps), max(lps), color=\"red\", linestyles=\"dashed\")\n",
        "axs[1].plot(xs, lps)\n",
        "axs[1].set_xlabel(\"log sigma\")\n",
        "axs[1].set_ylabel(\"log unnormalized posterior\\n(conditioned on \\\\hat\\\\mu)\")\n",
        "axs[1].text(1.0, max(lps), f\"log(sd(obs))): {jnp.log(jnp.std(obs)):.2f}\", ha=\"right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}